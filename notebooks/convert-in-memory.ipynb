{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde69193-94db-4295-9cc6-ff44a7b5089c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "\n",
    "from PIL import Image\n",
    "from urllib.request import urlopen\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "# //////////////////////////////////////////////////////\n",
    "def ShowImage(data):\n",
    "\tdata=np.flip(data,axis=0)\n",
    "\tprint(\"shape\",data.shape,\"dtype\",str(data.dtype))\n",
    "\tfig = plt.figure()\n",
    "\tax = fig.add_subplot(1,1,1)\n",
    "\tax.imshow(data, origin='lower')\n",
    "\tplt.show()\n",
    "\n",
    "sys.path.append(\"C:/projects/OpenVisus/build/RelWithDebInfo\")\n",
    "import OpenVisus as ov\n",
    "print(\"OpenVisus imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3457e2c7-8db4-464a-bcfd-e48e94a7e503",
   "metadata": {},
   "source": [
    "# Load original data from *.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import xarray as xr\n",
    "\n",
    "# Variable Name -> List of elevations\n",
    "variables = {\n",
    "  'precipitation': [0],  \n",
    "  'pressure': [0, 100, 200, 500],\n",
    "  'temperature': [2, 30, 40, 60, 80, 100, 200, 300, 500, 1000],\n",
    "  'virtual_potential_temperature': [2, 30, 40, 60, 80, 100, 200, 300, 500, 1000],\n",
    "  'winddirection': [10, 30, 32, 40, 44, 47, 54, 57, 60, 80, 100, 120, 140, 160, 180, 200, 250, 300, 500, 1000],\n",
    "  'windspeed': [10, 30, 32, 40, 44, 47, 54, 57, 60, 80, 100, 120, 140, 160, 180, 200, 250, 300, 500, 1000]\n",
    "}\n",
    "\n",
    "# Create fields for each variable and elevation\n",
    "fields = [\n",
    "  ov.Field('latitude',  'float32'), \n",
    "  ov.Field('longitude', 'float32'),\n",
    "]\n",
    "\n",
    "# Go through each variable and elevation combination\n",
    "for variable in variables:\n",
    "  for elevation in variables[variable]:\n",
    "    fields.append(ov.Field(f'{variable}_{elevation}m', \"int16\"))\n",
    "    \n",
    "nc_file = xr.open_dataset(\n",
    "  './cstm_d01_2016-05-01_00_00_00.nc', \n",
    "\n",
    "  # important because the original data is int16 (otherwise xarray cast )\n",
    "  # see https://docs.xarray.dev/en/stable/generated/xarray.open_dataset.html\n",
    "  mask_and_scale=False\n",
    "  )\n",
    "data={}\n",
    "shape=None\n",
    "for field in fields:\n",
    "  # print(field.name, data[field.name].shape,data[field.name].dtype)\n",
    "  # use isel(time=0) since each field is 3d but only has a depth of 1\n",
    "  data[field.name] = nc_file[field.name].isel(time=0).values\n",
    "  assert(shape is None or shape==data[field.name].shape)\n",
    "  shape=data[field.name].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd0bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if compressing using pure python gives back the same compression ratio\n",
    "np.savez_compressed(\"check-compression-size.npz\",data,compression=\"gzip\",compression_opts=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77cc199-e0bf-4287-8eb9-ec94c686f4cb",
   "metadata": {},
   "source": [
    "# Create IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b88f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import struct,io,time\n",
    "from io import BytesIO\n",
    "\n",
    "# since I am writing/reading in multiple threads, I will not have collisions and I will not need file locks\n",
    "os.environ[\"VISUS_DISABLE_WRITE_LOCK\"]=\"1\"\n",
    "\n",
    "# /////////////////////////////////////////////////////////////\n",
    "def CreateRamAccess(db):\n",
    "    ret=db.createAccess(ov.StringTree.fromString(\"<access type='RamAccess' chmod='rw' available='0' compression='raw'/>\"))\n",
    "    ret.disableWriteLocks()\n",
    "    assert(ret.bDisableWriteLocks==True)\n",
    "    assert(ret.compression==\"\")\n",
    "    assert(str(ret.getAccessTypeName())==\"class Visus::RamAccess\")\n",
    "    return ret\n",
    "\n",
    "dims=list(reversed(shape))\n",
    "\n",
    "# using a temporary filename , so I am sure I can remove the directory\n",
    "idx_filename='visus-remove-me/visus.idx'\n",
    "assert('visus-remove-me' in idx_filename)\n",
    "shutil.rmtree(os.path.dirname(idx_filename), ignore_errors=True)\n",
    "\n",
    "db=ov.CreateIdx(\n",
    "    url=idx_filename, \n",
    "    dims=dims, \n",
    "    fields=fields,\n",
    "    # writing at the beginning should be uncompressed \n",
    "    compression=\"raw\"\n",
    "    )\n",
    "\n",
    "# make sure the file has been created\n",
    "assert(os.path.isfile(idx_filename))\n",
    "\n",
    "blocksperfile=db.idxfile.blocksperfile\n",
    "total_blocks=db.getTotalNumberOfBlocks()\n",
    "\n",
    "nfields=len(fields)\n",
    "pdim=db.getPointDim()\n",
    "num_files=total_blocks//blocksperfile\n",
    "file_header_size=10*4\n",
    "block_header_size=10*4\n",
    "header0=np.zeros(shape=[file_header_size+nfields*blocksperfile*block_header_size,],dtype=np.uint8)\n",
    "assert(num_files*blocksperfile==total_blocks)\n",
    "\n",
    "print(f\"header0       ={header0.nbytes}\")\n",
    "print(f\"total_blocks  ={total_blocks}\")\n",
    "print(f\"pdim          ={pdim}\")\n",
    "print(f\"num_files     ={num_files}\")\n",
    "print(f\"dims          ={dims}\")\n",
    "print(f\"nfields       ={nfields}\")\n",
    "print(f\"blocksperfile ={blocksperfile}\")\n",
    "print(f\"header0       ={header0.nbytes}\")\n",
    "print(\"\")\n",
    "\n",
    "print(db.getDatasetBody().toString())\n",
    "\n",
    "# db.write(data,field=db.getField(\"temperature\"))\n",
    "# db.write(data,field=db.getField(\"pressure\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0db3ac-e3d2-4986-91d3-5cce65180b21",
   "metadata": {},
   "source": [
    "# Write data in memory\n",
    " - uncompressed\n",
    " - not using any disk IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3268a9-37d9-47e7-970e-cfc9401e1367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# by specifying an `access` I tell OpenVisus to write in memory and not on disk\n",
    "ram_access=CreateRamAccess(db)\n",
    "\n",
    "# for each timestep\n",
    "for timestep in db.getTimesteps():\n",
    "  \n",
    "  # for each field\n",
    "  for F,field in enumerate(db.getFields()):\n",
    "\n",
    "    # here I am writing the HDF5 (or numpy data, or whatever)\n",
    "    # this is called BoxQuery (equivalent to writing a Region of Interest)\n",
    "    print(f\"Writing timestep={timestep} field={field} F={F}...\",end=\"\")\n",
    "    logic_box=db.getLogicBox()\n",
    "    query = db.createBoxQuery(ov.BoxNi(ov.PointNi(logic_box[0]),ov.PointNi(logic_box[1])), db.getField(field), timestep, ord('w'))\n",
    "    db.beginBoxQuery(query)\n",
    "    assert(query.isRunning())\n",
    "    query.buffer = ov.Array.fromNumPy(data[field],TargetDim=pdim, bShareMem=True)\n",
    "    assert(db.executeBoxQuery(ram_access, query))\n",
    "    print(\"done\")\n",
    "\n",
    "# Here I am reding blocks (all in memory) and converting them into numpy arrays\n",
    "# using a dict to make block looking simplier\n",
    "blocks={}\n",
    "for timestep in db.getTimesteps():\n",
    "  for field in db.getFields():\n",
    "    for blockid in range(total_blocks):\n",
    "      read_block = db.createBlockQuery(blockid, db.getField(field), timestep, ord('r'))\n",
    "      if db.executeBlockQueryAndWait(ram_access, read_block): \n",
    "        # scrgiorgio: if I share the memory here I have a problem... why????\n",
    "        blocks[(timestep, field, blockid)]=ov.Array.toNumPy(read_block.buffer, bShareMem=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa198188",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# verify\n",
    "if True:\n",
    "  for field in db.getFields():\n",
    "    check=db.read(time=timestep,field=db.getField(field), access=ram_access)\n",
    "    assert(np.array_equal(data[field], check))\n",
    "    # ShowImage(check)\n",
    "  print(\"Verification ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac14fbc9",
   "metadata": {},
   "source": [
    "# Write blocks to disk using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf62d9-f2e6-49e7-ac9a-cd825bfb31f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import zlib\n",
    "\n",
    "NoCompression   = 0x00\n",
    "ZipMask         = 0x03\n",
    "ZfpCompression  = 0x08\n",
    "CompressionMask = 0x0f\n",
    "\n",
    "RowMajor        = 0x10\n",
    "\n",
    "# use empty string or `zip` here\n",
    "compression=\"zip\"\n",
    "\n",
    "# TODO (!)\n",
    "assert(compression!=\"zfp\") \n",
    "\n",
    "# at the beginning I am using a disk access to generate filename, later on to verify the data\n",
    "disk_access=db.createAccess()\n",
    "\n",
    "#using bytesIO does not help at all (scrgiorgio: at least from today tests 20240527)\n",
    "use_bytes_io=False\n",
    "\n",
    "# block statistics\n",
    "statistics={}\n",
    "\n",
    "# for each timestep\n",
    "for timestep in db.getTimesteps():\n",
    "\n",
    "\t# for each block inside file\n",
    "\tfor fileid, BLOCKID in enumerate(range(0,total_blocks,blocksperfile)):\n",
    "\n",
    "\t\t# create the file\n",
    "\t\tfilename=disk_access.getFilename(db.getField(), timestep, BLOCKID)\n",
    "\t\t\n",
    "\t\tos.makedirs(os.path.dirname(filename),exist_ok =True)\n",
    "\t\tif os.path.isfile(filename):\n",
    "\t\t\tos.remove(filename)\n",
    "\t\t\n",
    "\t\t# using BytesIO does not seem to help at all\n",
    "\t\twith (BytesIO() if use_bytes_io else open(filename,\"wb\")) as stream:\n",
    "\n",
    "\t\t\tprint(f\"Writing filename={filename}...\")\n",
    "\n",
    "\t\t\tstream.write(header0.tobytes())\n",
    "\t\t\tfor field_index,field in enumerate(db.getFields()):\n",
    "\t\t\t\tfor B,blockid in enumerate(range(BLOCKID,BLOCKID+blocksperfile)):\n",
    "\n",
    "\t\t\t\t\tkey=(timestep,field,blockid)\n",
    "\t\t\t\t\tblock=blocks.get(key,None)\n",
    "\t\t\t\t\tif block is None:  continue\n",
    "\n",
    "\t\t\t\t\t# i do not want to write all zero blocks\n",
    "\t\t\t\t\tall_zeros = not np.any(block)\n",
    "\t\t\t\t\tif all_zeros:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\tflags=RowMajor\n",
    "\n",
    "\t\t\t\t\t# no need to compress\n",
    "\t\t\t\t\tif compression in [\"\",\"raw\"]:\n",
    "\t\t\t\t\t\tblock=block.tobytes()\n",
    "\t\t\t\t\t\tflags|=0\n",
    "\n",
    "\t\t\t\t\t# need to compress in zip\n",
    "\t\t\t\t\telif compression==\"zip\":\n",
    "\t\t\t\t\t\tblock=zlib.compress(block,level=9)\n",
    "\t\t\t\t\t\tflags|=ZipMask\n",
    "\n",
    "\t\t\t\t\t# write the binary data\n",
    "\t\t\t\t\tstream.seek(0, io.SEEK_END)\n",
    "\t\t\t\t\toffset=stream.tell()\n",
    "\t\t\t\t\tsize=len(block)\n",
    "\t\t\t\t\tstream.write(block)\n",
    "\n",
    "\t\t\t\t\t# write the file header\n",
    "\t\t\t\t\tstream.seek(file_header_size+block_header_size*(field_index*blocksperfile+B), io.SEEK_SET)\n",
    "\t\t\t\t\tstream.write(struct.pack('>IIIIIIIIII', 0,0, offset & 0xffffffff, offset>>32, size, flags,0,0,0,0))\n",
    "\n",
    "\t\t\t\t\t# print(f\"Wrote block timestep={timestep} field={field} field_index={field_index} blockid={blockid} B={B} offset={offset} size={size} flags={flags}\")\n",
    "\t\t\t\t\tstatistics[key]=(offset,size)\n",
    "\n",
    "\t\t\t# need to do the real IO \n",
    "\t\t\tif use_bytes_io:\n",
    "\t\t\t\twith open(filename, \"wb\") as f:\n",
    "\t\t\t\t\tf.write(stream.getbuffer())\t\n",
    "\n",
    "\t\t\tprint(f\"Writing filename={filename} DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f85098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "\tfor field in db.getFields():\n",
    "\t\tcheck=db.read(field=field, access=disk_access)\n",
    "\t\tassert(np.array_equal(data[field], check))\n",
    "\t\t# ShowImage(check)\n",
    "\tprint(\"Verification ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9e77d",
   "metadata": {},
   "source": [
    "# Plot block statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = px.data.tips()\n",
    "\n",
    "size_int16=2\n",
    "uncompressed_single_timestep_size=np.product(shape)*size_int16*len(db.getFields())\n",
    "uncompressed_block_size=(1<<db.getDefaultBitsPerBlock())*size_int16\n",
    "print(f\"uncompressed_single_timestep_size        ={uncompressed_single_timestep_size:,}\")\n",
    "print(f\"uncompressed_block_size                  ={uncompressed_block_size:,}\")\n",
    "\n",
    "v=[]\n",
    "tot=0\n",
    "for key,(offset,size) in statistics.items():\n",
    "  tot+=size\n",
    "  v.append(100*size/uncompressed_block_size)\n",
    "\n",
    "print(f\"% ratio ={100*tot/uncompressed_single_timestep_size}\")\n",
    "\n",
    "fig = px.histogram(v)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0c082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{480215934*0.88:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b8f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install --quiet h5glance\n",
    "#from h5glance import H5Glance\n",
    "#H5Glance('./cstm_d01_2016-05-01_00_00_00.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2897afa8",
   "metadata": {},
   "source": [
    "# ZFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e684694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have only one writer\n",
    "os.environ[\"VISUS_DISABLE_WRITE_LOCK\"]=\"1\"\n",
    "\n",
    "data=np.load(\"./recon_combined_1_fullres.npy\")\n",
    "depth,height,width=data.shape\n",
    "print(f\"np.load done dtype={data.dtype} shape={data.shape} c_size={width*height*depth*4:,}\")\n",
    "\n",
    "idx_filename=\"remove-me/zfp/visus.idx\"\n",
    "shutil.rmtree(os.path.dirname(idx_filename), ignore_errors=True)\n",
    "fields=[ov.Field(\"data\",str(data.dtype),\"row_major\")]\n",
    "db=ov.CreateIdx(url=idx_filename,dims=[width,height,depth],fields=fields,compression=\"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1cc09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.write(data)\n",
    "print(f\"db.write (uncompressed) done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0875c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding-number-of-bits and decoding-number-of-bits\n",
    "#   it will be written in the IDX file and used as the field.default_compression\n",
    "#   this is needed since the IDX block header does not support/store number-of-bitblanes\n",
    "t1 = time.time()\n",
    "db.compressDataset([\"zfp-precision=8-precision=8\"]) \n",
    "print(f\"db.compressDataset done in {time.time()-t1} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dcda4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
